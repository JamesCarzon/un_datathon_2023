---
title: "GP_modeling"
author: "James Carzon"
date: "11/4/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{python}
import numpy as npZ
import pandas as pd
import sklearn.gaussian_process as gp
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import scipy
import matplotlib.pyplot as plt
```

```{python}
data = pd.read_csv("../DATA/processed_data.csv", index_col=0)
data['elevation'] = 10*scipy.stats.norm.rvs(size=data.shape[0]) # Fake elevation values
```

We specify a Matern kernel with shape parameter 1.5 and with a vector of 
initial length scales equal to 1.

```{python}
kernel = gp.kernels.Matern(
  length_scale=[1.0]*5,
  length_scale_bounds=(10e-10, 10e10),
  nu=1.5
)

model = gp.GaussianProcessRegressor(
  kernel,
  optimizer='fmin_l_bfgs_b',
  random_state=2023
)
```

Specify input data X and y. Scale data and then perform 70-30 train-test split.

```{python}
X = data[['latitude', 'longitude', 'popdens', 'elevation', 'plant_dist']]
y = data[['NO2gm']]

scaler = StandardScaler()
scaler.fit(X)
print(scaler.mean_)
X_scaled = scaler.transform(X)

# Check pre-scaled vs scaled shapes
X.shape
X_scaled.shape

X_train, X_test, y_train, y_test = train_test_split(
  X_scaled, y, test_size=0.30, random_state=2023)
  
# Check train-test shapes

```

Fit the model by learning the length scales by MLE, and read the learned 
parameters.

```{python}
t0 = time.time()
model.fit(X_train, y_train, max)
t1 = time.time()
print(t1 - t0)

model.kernel_.get_params()
```
